{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K7LJRx4QBv6"
      },
      "source": [
        "AI statement:\n",
        "\"I certify that the code and data in this assignment were generated independently, using only the tools\n",
        "and resources defined in the course and that I did not receive any external help, coaching, or contributions\n",
        "during the production of this work.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vYSnWaCQBv9"
      },
      "source": [
        "### Part I: Exploring Gym Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmtgbWohzhMw"
      },
      "source": [
        "#### Cartpole-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSNjECbtpbzA",
        "outputId": "c24b631f-18b7-4140-8dbe-b04a3d140f64"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "!python -m pip install pyvirtualdisplay\n",
        "!pip install box2d\n",
        "!apt-get install xvfb -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "hE80Z60fpeJv",
        "outputId": "915b652c-4422-4a92-8690-35cac646f00e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(backend='xvfb', visible=False, size=(400, 300), manage_global_env=True)\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in plt.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# Load the gym environment\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(23)\n",
        "\n",
        "# Let's watch how an untrained agent moves around\n",
        "\n",
        "state = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "for j in range(20000):\n",
        "#     action = agent.act(state)\n",
        "    action = random.choice(range(2))\n",
        "    img.set_data(env.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHMoM5yrizXh"
      },
      "source": [
        "Read Environment Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp-BGQKpwCEH",
        "outputId": "4a28ffb0-1983-4d94-ff6e-e0d4b2465a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "''' Action Space\n",
        "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
        "    | Num | Action                 |\n",
        "    |-----|------------------------|\n",
        "    | 0   | Push cart to the left  |\n",
        "    | 1   | Push cart to the right |\n",
        "'''\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE5ZHvjFwPFs",
        "outputId": "c0797db7-5a65-46f3-80eb-fc2c4e2816dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "    ''' Observation Space\n",
        "        The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "        | Num | Observation           | Min                  | Max                |\n",
        "        |-----|-----------------------|----------------------|--------------------|\n",
        "        | 0   | Cart Position         | -4.8                 | 4.8                |\n",
        "        | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
        "        | 2   | Pole Angle            | ~ -0.418 rad (-24°)  | ~ 0.418 rad (24°)  |\n",
        "        | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
        "    '''\n",
        "    print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL7fimRvxheH"
      },
      "source": [
        "**Rewards**  \n",
        "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.<br><br>\n",
        "**Starting State**  \n",
        "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`<br><br>\n",
        "**Episode Termination**  \n",
        "    The episode terminates if any one of the following occurs:<br>\n",
        "1.   Pole Angle is greater than ±12°\n",
        "2.   Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3.   Episode length is greater than 500 (200 for v0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XupR2NxFz0TJ"
      },
      "source": [
        "#### MountainCar-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "69kU0jr30En5",
        "outputId": "089071d1-3db2-4045-e4d9-a838f529c4c7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "env2 = gym.make('MountainCar-v0')\n",
        "env2.seed(23)\n",
        "env2.reset()\n",
        "\n",
        "# Let's watch how an untrained agent moves around\n",
        "\n",
        "state = env.reset()\n",
        "img = plt.imshow(env2.render(mode='rgb_array'))\n",
        "for j in range(20000):\n",
        "#     action = agent.act(state)\n",
        "    action = random.choice(range(2))\n",
        "    img.set_data(env2.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    state, reward, done, _ = env2.step(action)\n",
        "    if done:\n",
        "        break \n",
        "        \n",
        "env2.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FbwV5kY0mgI",
        "outputId": "be54d085-3e20-41a2-953b-b8c48030e587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(3)\n"
          ]
        }
      ],
      "source": [
        "''' Action Space\n",
        "    There are 3 discrete deterministic actions:\n",
        "    | Num | Observation                                                 | Value   | Unit |\n",
        "    |-----|-------------------------------------------------------------|---------|------|\n",
        "    | 0   | Accelerate to the left                                      | Inf    | position (m) |\n",
        "    | 1   | Don't accelerate                                            | Inf  | position (m) |\n",
        "    | 2   | Accelerate to the right                                     | Inf    | position (m) |\n",
        "'''\n",
        "print(env2.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxE6w8xf10O3",
        "outputId": "b8ce0a2c-c9d7-4d7f-eadb-cbe93694277c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
          ]
        }
      ],
      "source": [
        "    ''' Observation Space\n",
        "        The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
        "        | Num | Observation                                                 | Min                | Max    | Unit |\n",
        "        |-----|-------------------------------------------------------------|--------------------|--------|------|\n",
        "        | 0   | position of the car along the x-axis                        | -Inf               | Inf    | position (m) |\n",
        "        | 1   | velocity of the car                                         | -Inf               | Inf  | position (m) |\n",
        "    '''\n",
        "    print(env2.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glwUddcT2vFg"
      },
      "source": [
        "**Reward**  \n",
        "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep it isn't at the goal and is not penalised (reward = 0) for when it reaches the goal.<br><br>\n",
        "**Starting State**  \n",
        "    The position of the car is assigned a uniform random value in [-0.6 , -0.4]. The starting velocity of the car is always assigned to 0.<br><br>\n",
        "**Episode Termination**  \n",
        "The episode terminates if either of the following happens:\n",
        "1. The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
        "2. The length of the episode is 200."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVIQ5OwW3son"
      },
      "source": [
        "### Part II: Implementing DQN & Solving grid-world environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ts29crCe7Wl-"
      },
      "outputs": [],
      "source": [
        "from gym import spaces\n",
        "import pandas as pd\n",
        "import copy as cp\n",
        "import math\n",
        "import numpy as np\n",
        "import io\n",
        "from base64 import b64decode, b64encode\n",
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZZ5J_ljg6HLr"
      },
      "outputs": [],
      "source": [
        "class GridEnvironment(gym.Env):\n",
        "  metadata = { 'render.modes': []}\n",
        "\n",
        "  def __init__(self, observation_space, action_space, max_timesteps, stochasticity=1):\n",
        "    self.observation_space = spaces.Discrete(observation_space)\n",
        "    self.action_space = spaces.Discrete(action_space)\n",
        "    self.max_timesteps = max_timesteps\n",
        "    self.stochasticity = stochasticity\n",
        "\n",
        "  def set_epsilon(self, epsilon):\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def reset(self):\n",
        "    self.timestep = 0\n",
        "    self.agent_pos = [0, 0]\n",
        "    self.goal_pos = [3, 3]\n",
        "    self.state = np.zeros((4, 4))\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.state[tuple(self.goal_pos)] = 0.5\n",
        "    observation = self.state.flatten()\n",
        "    return observation\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    if self.stochasticity != 1:\n",
        "      action = self.get_action_random(action)\n",
        "\n",
        "    current_pos = cp.deepcopy(self.agent_pos)\n",
        "\n",
        "    if action == 0: # Go left\n",
        "      self.agent_pos[0] -= 1\n",
        "    if action == 1: # Go up\n",
        "      self.agent_pos[1] -= 1\n",
        "    if action == 2: # Go right\n",
        "      self.agent_pos[0] += 1\n",
        "    if action == 3: # Go down\n",
        "      self.agent_pos[1] += 1\n",
        "    \n",
        "    self.agent_pos = list(np.clip(self.agent_pos, 0, 3))\n",
        "    future_pos = self.agent_pos\n",
        "    self.state = np.zeros((4,4))\n",
        "    self.state[tuple(self.agent_pos)] = 1\n",
        "    self.state[tuple(self.goal_pos)] = 0.5\n",
        "    observation = self.state.flatten()\n",
        "\n",
        "    reward = self.get_reward(current_pos, future_pos)\n",
        "\n",
        "    self.timestep += 1\n",
        "    done = True if self.timestep >= self.max_timesteps or self.agent_pos == self.goal_pos else False\n",
        "    info = {}\n",
        "\n",
        "    return observation, reward, done, info, action\n",
        "\n",
        "  # the reward is calculated on the basis of the distance of the\n",
        "  # current_pos and future_pos of the agent from the goal_pos\n",
        "  def get_reward(self, current_pos, future_pos):\n",
        "\n",
        "    current_dist = self.distance_from_goal(current_pos)\n",
        "    final_dist = self.distance_from_goal(future_pos)\n",
        "\n",
        "    if final_dist < current_dist:\n",
        "      return 1\n",
        "    else:\n",
        "      return -1\n",
        "  \n",
        "  def distance_from_goal(self, pos):\n",
        "    x_final, y_final = self.goal_pos\n",
        "    x, y = pos\n",
        "    return math.sqrt( (y_final - y)**2 + (x_final - x)**2 )\n",
        "\n",
        "  def get_action_random(self, action):\n",
        "    if action == 1:\n",
        "      return 3\n",
        "    else:\n",
        "      return action\n",
        "      \n",
        "\n",
        "  # https://stackoverflow.com/questions/25698448/how-to-embed-html-into-ipython-output\n",
        "  # https://stackoverflow.com/questions/55562088/open-base64-string-image-in-jupyter-notebook-without-saving\n",
        "  # https://stackoverflow.com/questions/49015957/how-to-get-python-graph-output-into-html-webpage-directly\n",
        "  # https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "  def render(self):\n",
        "    plt.figure()\n",
        "    # plt.imshow(self.state)\n",
        "    img = io.BytesIO()\n",
        "    plt.imsave(img, self.state)\n",
        "    img.seek(0)\n",
        "    img_val = img.getvalue()\n",
        "    # print(img_val)\n",
        "    encoded_image = b64encode(img_val)\n",
        "    my_html = '<img height=\"225px\" width=\"300px\" src=\"data:image/png;base64, {}\">'.format(encoded_image.decode('utf-8'))\n",
        "    display(HTML('<h1>' + my_html + '</h1>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdFVrOh28LUK"
      },
      "source": [
        "The following function returns the action the agent 🤖 will take"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "wi0wRi41vVCL",
        "outputId": "c95342dc-82b6-408e-d335-26fc444c789e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3838/1825466926.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochasticity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3838/258072498.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mencoded_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmy_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<img height=\"225px\" width=\"300px\" src=\"data:image/png;base64, {}\">'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<h1>'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmy_html\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'</h1>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "stochasticity = 1.0\n",
        "max_timesteps = 15\n",
        "observation_space = 16\n",
        "action_space = 4\n",
        "max_timesteps = 10\n",
        "\n",
        "env = GridEnvironment(observation_space, action_space, max_timesteps, stochasticity=1)\n",
        "obs = env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9H6KtQIw8MCh"
      },
      "outputs": [],
      "source": [
        "def get_action():\n",
        "    return np.random.choice(env.action_space.n)\n",
        "\n",
        "def one_hot(x, depth: int):\n",
        "  return np.take(np.eye(depth), x, axis=0).astype('double')\n",
        "\n",
        "def get_state_from_position(pos):\n",
        "    X, Y = env.state.shape\n",
        "    pos_x, pos_y = pos\n",
        "    return Y * pos_x + pos_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tZbvqvGT8Sic"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# l1 = 16 as it will receive the one hot encoded observation space\n",
        "l1 = 16\n",
        "l2 = 150\n",
        "l3 = 100\n",
        "l4 = 4\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(l1, l2),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(l2, l3),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(l3,l4)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "gamma = 0.9\n",
        "epsilon = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "DRxpPvVom878",
        "outputId": "8ddc8144-f17d-401e-b3f0-7dc02f4eccfa"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3838/2462013124.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m# ToDo: The below needs to be looped properly to create the respective minibatches for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# state1_batch = torch.cat([s1 for [s1,a,r,s2,d] in minibatch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mstate1_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
          ]
        }
      ],
      "source": [
        "batch_length = 5\n",
        "\n",
        "epochs = 2000\n",
        "losses = []\n",
        "mem_size = 1000 * batch_length                                                            \n",
        "batch_size = 200 * batch_length                                                                   \n",
        "# replay = deque(maxlen=mem_size)\n",
        "replay = np.array([])\n",
        "max_moves = 50                                                                 \n",
        "h = 0\n",
        "\n",
        "\n",
        "debug = True\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  env.reset()\n",
        "\n",
        "  completed = False\n",
        "\n",
        "  while not completed:\n",
        "\n",
        "    current_state = get_state_from_position(env.agent_pos)\n",
        "    current_state_oh = one_hot(current_state, depth=16).astype(np.float32)\n",
        "    current_state_oh = torch.from_numpy(current_state_oh)\n",
        "    qval = model(current_state_oh)                                                   \n",
        "    qval_ = qval.data.numpy()\n",
        "\n",
        "    if (random.random() < epsilon):                                       \n",
        "      current_action = get_action()\n",
        "    else:\n",
        "      current_action = np.argmax(qval_)\n",
        "\n",
        "    future_state = get_state_from_position(env.agent_pos)\n",
        "    observation, reward, done, info, action = env.step(current_action)\n",
        "    \n",
        "    record = np.array([current_state, current_action, reward, future_state, done])\n",
        "    \n",
        "    if len(replay) == mem_size:\n",
        "      replay = np.delete(replay, replay[0:5], axis=0)\n",
        "\n",
        "    replay = np.append(replay, record, axis=0)\n",
        "\n",
        "\n",
        "    if len(replay) >= batch_size:\n",
        "      rng = np.random.default_rng()\n",
        "      minibatch = rng.choice(replay, batch_size)\n",
        "      \n",
        "      minibatch = np.reshape(minibatch, (len(replay)// batch_length, batch_length))\n",
        "\n",
        "      # ToDo: The below needs to be looped properly to create the respective minibatches for training\n",
        "      state1_batch = torch.cat([s1 for [s1,a,r,s2,d] in minibatch])      \n",
        "      state1_batch = torch.cat([s[0] for s in minibatch])\n",
        "      action_batch = torch.Tensor([a for [s1,a,r,s2,d] in minibatch])\n",
        "      reward_batch = torch.Tensor([r for [s1,a,r,s2,d] in minibatch])\n",
        "      state2_batch = torch.cat([s2 for [s1,a,r,s2,d] in minibatch])\n",
        "      done_batch = torch.Tensor([d for [s1,a,r,s2,d] in minibatch])\n",
        "      \n",
        "      Q1 = model(state1_batch)                                           \n",
        "      with torch.no_grad():\n",
        "          Q2 = model(state2_batch)                                      \n",
        "      \n",
        "      Y = reward_batch + gamma * ((1 - done_batch) * torch.max(Q2,dim=1)[0])\n",
        "      X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "      loss = loss_fn(X, Y.detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      losses.append(loss.item())\n",
        "      optimizer.step()\n",
        "      \n",
        "\n",
        "    completed = True if done else False\n",
        "\n",
        "losses = np.array(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     game = GridWorld(size=4, mode='random')\n",
        "#     state1_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0\n",
        "#     state1 = torch.from_numpy(state1_).float()\n",
        "#     status = 1\n",
        "#     mov = 0\n",
        "#     while(status == 1): \n",
        "#         mov += 1\n",
        "#         qval = model(state1)                                                   \n",
        "#         qval_ = qval.data.numpy()\n",
        "#         if (random.random() < epsilon):                                       \n",
        "#             action_ = np.random.randint(0,4)\n",
        "#         else:\n",
        "#             action_ = np.argmax(qval_)\n",
        "        \n",
        "#         action = action_set[action_]\n",
        "#         game.makeMove(action)\n",
        "#         state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0\n",
        "#         state2 = torch.from_numpy(state2_).float()\n",
        "#         reward = game.reward()\n",
        "#         done = True if reward > 0 else False\n",
        "#         exp =  (state1, action_, reward, state2, done)                        \n",
        "#         replay.append(exp)                                                    \n",
        "#         state1 = state2\n",
        "#         if len(replay) > batch_size:                                          \n",
        "#             minibatch = random.sample(replay, batch_size)                    \n",
        "#             state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])      \n",
        "#             action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])\n",
        "#             reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])\n",
        "#             state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])\n",
        "#             done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])\n",
        "            \n",
        "#             Q1 = model(state1_batch)                                           \n",
        "#             with torch.no_grad():\n",
        "#                 Q2 = model(state2_batch)                                      \n",
        "            \n",
        "#             Y = reward_batch + gamma * ((1 - done_batch) * torch.max(Q2,dim=1)[0])\n",
        "#             X = \\\n",
        "#             Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "#             loss = loss_fn(X, Y.detach())\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             losses.append(loss.item())\n",
        "#             optimizer.step()\n",
        " \n",
        "#         if reward != -1 or mov > max_moves:                                    \n",
        "#             status = 0\n",
        "#             mov = 0\n",
        "# losses = np.array(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HmtgbWohzhMw"
      ],
      "name": "RL-Assignment-2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7c3b01d34565c9fdde920b12ba7e9f817cc93765a4c91bd25da05fa7939268b5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('pytorch_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
